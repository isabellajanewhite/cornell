import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv('/Users/isabellawhite/Desktop/Cornell/Machine Learning/Data_supermarket_sales.csv')
df.head()
Invoice_ID	Branch	City	Customer_type	Gender	Gender.1	Product_line	Unit_price	Quantity	Tax.05	Total	Date	Time	Payment	cogs	gross_margin_percentage	gross_income	Rating
0	750-67-8428	A	Yangon	Member	Female	1	Health and beauty	74.69	7	26.1415	548.9715	1/5/2019	13:08	Ewallet	522.83	4.761905	26.1415	9.1
1	226-31-3081	C	Naypyitaw	Normal	Female	1	Electronic accessories	15.28	5	3.8200	80.2200	3/8/2019	10:29	Cash	76.40	4.761905	3.8200	9.6
2	631-41-3108	A	Yangon	Normal	Male	0	Home and lifestyle	46.33	7	16.2155	340.5255	3/3/2019	13:23	Credit card	324.31	4.761905	16.2155	7.4
3	123-19-1176	A	Yangon	Member	Male	0	Health and beauty	58.22	8	23.2880	489.0480	1/27/2019	20:33	Ewallet	465.76	4.761905	23.2880	8.4
4	373-73-7910	A	Yangon	Normal	Male	0	Sports and travel	86.31	7	30.2085	634.3785	2/8/2019	10:37	Ewallet	604.17	4.761905	30.2085	5.3
df.describe()
Gender.1	Unit_price	Quantity	Tax.05	Total	cogs	gross_margin_percentage	gross_income	Rating
count	1000.000000	1000.000000	1000.000000	1000.000000	1000.000000	1000.00000	1.000000e+03	1000.000000	1000.00000
mean	0.501000	55.672130	5.510000	15.379369	322.966749	307.58738	4.761905e+00	15.379369	6.97270
std	0.500249	26.494628	2.923431	11.708825	245.885335	234.17651	6.131498e-14	11.708825	1.71858
min	0.000000	10.080000	1.000000	0.508500	10.678500	10.17000	4.761905e+00	0.508500	4.00000
25%	0.000000	32.875000	3.000000	5.924875	124.422375	118.49750	4.761905e+00	5.924875	5.50000
50%	1.000000	55.230000	5.000000	12.088000	253.848000	241.76000	4.761905e+00	12.088000	7.00000
75%	1.000000	77.935000	8.000000	22.445250	471.350250	448.90500	4.761905e+00	22.445250	8.50000
max	1.000000	99.960000	10.000000	49.650000	1042.650000	993.00000	4.761905e+00	49.650000	10.00000
table = pd.pivot_table(data=df,index=['Gender'])
print(table)
        Gender.1  Quantity    Rating     Tax.05       Total  Unit_price  \
Gender                                                                    
Female         1  5.726547  6.964471  15.956936  335.095659   55.263952   
Male           0  5.292585  6.980962  14.799487  310.789226   56.081944   

              cogs  gross_income  gross_margin_percentage  
Gender                                                     
Female  319.138723     15.956936                 4.761905  
Male    295.989739     14.799487                 4.761905  
print(table)
        Gender.1  Quantity    Rating     Tax.05       Total  Unit_price  \
Gender                                                                    
Female         1  5.726547  6.964471  15.956936  335.095659   55.263952   
Male           0  5.292585  6.980962  14.799487  310.789226   56.081944   

              cogs  gross_income  gross_margin_percentage  
Gender                                                     
Female  319.138723     15.956936                 4.761905  
Male    295.989739     14.799487                 4.761905  
import seaborn as sns

# Group the DataFrame by the desired categorical variables and calculate the sum of 'Total'
grouped_data = df.groupby(['Gender', 'City', 'Customer_type', 'Branch', 'Product_line', 'Payment'])['Total'].sum().reset_index()

# Plotting the sum of 'Total' by Gender using a barplot
plt.figure(figsize=(12, 6))
sns.barplot(x='Total', y='Gender', hue='City', data=grouped_data)
plt.title('Sum of Total by Gender and City')
plt.xlabel('Sum of Total')
plt.ylabel('Gender')
plt.show()

# Plotting the sum of 'Total' by Customer Type using a barplot
plt.figure(figsize=(12, 6))
sns.barplot(x='Total', y='Customer_type', hue='City', data=grouped_data)
plt.title('Sum of Total by Customer Type and City')
plt.xlabel('Sum of Total')
plt.ylabel('Customer Type')
plt.show()

# Plotting the sum of 'Total' by Product Line using a barplot
plt.figure(figsize=(12, 6))
sns.barplot(x='Total', y='Product_line', hue='City', data=grouped_data)
plt.title('Sum of Total by Product Line and City')
plt.xlabel('Sum of Total')
plt.ylabel('Product Line')
plt.show()

# Plotting the sum of 'Total' by Payment using a barplot
plt.figure(figsize=(12, 6))
sns.barplot(x='Total', y='Payment', hue='City', data=grouped_data)
plt.title('Sum of Total by Payment and City')
plt.xlabel('Sum of Total')
plt.ylabel('Payment')
plt.show()




from tabpy.tabpy_tools.client import Client

# Connect to TabPy server
client = Client('http://localhost:9004/')

-----


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv('/Users/isabellawhite/Desktop/Cornell/Machine Learning/Data_supermarket_sales.csv')
import statsmodels.api as sm

encoded_df = pd.get_dummies(df[['Gender', 'Branch', 'City', 'Customer_type', 'Payment']])

# Concatenate the encoded variables with the Total column
X = pd.concat([encoded_df, df['Total']], axis=1)

# Separate the independent variables (X) and the dependent variable (y)
y = X['Total']
X = X.drop('Total', axis=1)

# Add a constant column to the independent variables matrix
X = sm.add_constant(X)

# Perform multiple regression
regression = sm.OLS(y, X)
results_total = regression.fit()

# Print the regression summary
print(results_total.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  Total   R-squared:                       0.003
Model:                            OLS   Adj. R-squared:                 -0.003
Method:                 Least Squares   F-statistic:                    0.5722
Date:                Tue, 11 Jul 2023   Prob (F-statistic):              0.753
Time:                        19:41:36   Log-Likelihood:                -6921.6
No. Observations:                1000   AIC:                         1.386e+04
Df Residuals:                     993   BIC:                         1.389e+04
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
========================================================================================
                           coef    std err          t      P>|t|      [0.025      0.975]
----------------------------------------------------------------------------------------
const                -4.975e+14   1.33e+15     -0.375      0.708    -3.1e+15    2.11e+15
Gender_Female          -2.3e+14   6.39e+14     -0.360      0.719   -1.48e+15    1.02e+15
Gender_Male            -2.3e+14   6.39e+14     -0.360      0.719   -1.48e+15    1.02e+15
Branch_A              2.569e+15   3.09e+15      0.830      0.407    -3.5e+15    8.64e+15
Branch_B               6.48e+14   8.44e+14      0.768      0.443   -1.01e+15     2.3e+15
Branch_C              5.558e+14   7.25e+14      0.766      0.444   -8.67e+14    1.98e+15
City_Mandalay         5.417e+14   7.14e+14      0.759      0.448   -8.59e+14    1.94e+15
City_Naypyitaw        6.339e+14   8.32e+14      0.762      0.446   -9.99e+14    2.27e+15
City_Yangon          -1.379e+15   1.67e+15     -0.827      0.409   -4.65e+15    1.89e+15
Customer_type_Member -2.263e+14   8.18e+14     -0.277      0.782   -1.83e+15    1.38e+15
Customer_type_Normal -2.263e+14   8.18e+14     -0.277      0.782   -1.83e+15    1.38e+15
Payment_Cash         -2.359e+14   4.55e+14     -0.518      0.604   -1.13e+15    6.57e+14
Payment_Credit card  -2.359e+14   4.55e+14     -0.518      0.604   -1.13e+15    6.57e+14
Payment_Ewallet      -2.359e+14   4.55e+14     -0.518      0.604   -1.13e+15    6.57e+14
==============================================================================
Omnibus:                       98.171   Durbin-Watson:                   1.933
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              127.343
Skew:                           0.872   Prob(JB):                     2.23e-28
Kurtosis:                       2.892   Cond. No.                     9.28e+18
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 3.48e-35. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
Regression analysis using a dependent variable of sales quantity.

Regression analysis using a dependent variable of total sales.

# Coefficient Plot
plt.figure(figsize=(20, 2))
coef_values = results_total.params.drop('const')
coef_values.plot(kind='bar')
plt.title('Sales Total Regression Coefficient Plot')
plt.xlabel('Independent Variables')
plt.ylabel('Coefficient')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()
import statsmodels.api as sm

encoded_df = pd.get_dummies(df[['Gender', 'Branch', 'City', 'Customer_type', 'Payment']])

# Concatenate the encoded variables with the Total column
X = pd.concat([encoded_df, df['Quantity']], axis=1)

# Separate the independent variables (X) and the dependent variable (y)
y = X['Quantity']
X = X.drop('Quantity', axis=1)

# Add a constant column to the independent variables matrix
X = sm.add_constant(X)

# Perform multiple regression
regression = sm.OLS(y, X)
results_quantity = regression.fit()

# Print the regression summary
print(results_quantity.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:               Quantity   R-squared:                       0.005
Model:                            OLS   Adj. R-squared:                 -0.001
Method:                 Least Squares   F-statistic:                    0.7980
Date:                Tue, 11 Jul 2023   Prob (F-statistic):              0.572
Time:                        19:41:58   Log-Likelihood:                -2488.8
No. Observations:                1000   AIC:                             4992.
Df Residuals:                     993   BIC:                             5026.
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
========================================================================================
                           coef    std err          t      P>|t|      [0.025      0.975]
----------------------------------------------------------------------------------------
const                -2.364e+11   1.58e+13     -0.015      0.988   -3.12e+13    3.07e+13
Gender_Female         6.121e+10    7.6e+12      0.008      0.994   -1.48e+13     1.5e+13
Gender_Male           6.121e+10    7.6e+12      0.008      0.994   -1.48e+13     1.5e+13
Branch_A              3.584e+13   3.68e+13      0.975      0.330   -3.63e+13    1.08e+14
Branch_B              7.602e+12      1e+13      0.758      0.448   -1.21e+13    2.73e+13
Branch_C              6.499e+12   8.62e+12      0.754      0.451   -1.04e+13    2.34e+13
City_Mandalay         6.234e+12   8.48e+12      0.735      0.462   -1.04e+13    2.29e+13
City_Naypyitaw        7.337e+12   9.89e+12      0.742      0.458   -1.21e+13    2.67e+13
City_Yangon            -2.2e+13   1.98e+13     -1.110      0.267   -6.09e+13    1.69e+13
Customer_type_Member -8.009e+12   9.72e+12     -0.824      0.410   -2.71e+13    1.11e+13
Customer_type_Normal -8.009e+12   9.72e+12     -0.824      0.410   -2.71e+13    1.11e+13
Payment_Cash         -5.652e+12   5.41e+12     -1.045      0.296   -1.63e+13    4.96e+12
Payment_Credit card  -5.652e+12   5.41e+12     -1.045      0.296   -1.63e+13    4.96e+12
Payment_Ewallet      -5.652e+12   5.41e+12     -1.045      0.296   -1.63e+13    4.96e+12
==============================================================================
Omnibus:                      723.524   Durbin-Watson:                   1.893
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               59.828
Skew:                           0.012   Prob(JB):                     1.02e-13
Kurtosis:                       1.802   Cond. No.                     9.28e+18
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 3.48e-35. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
# Coefficient Plot
plt.figure(figsize=(20, 2))
coef_values = results_quantity.params.drop('const')
coef_values.plot(kind='bar')
plt.title('Quantity Regression Coefficient Plot')
plt.xlabel('Independent Variables')
plt.ylabel('Coefficient')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

Regression analysis using a dependent variable of customer satisfaction rating.

import statsmodels.api as sm

encoded_df = pd.get_dummies(df[['Gender', 'Branch', 'City', 'Customer_type', 'Payment']])

# Concatenate the encoded variables with the Total column
X = pd.concat([encoded_df, df['Rating']], axis=1)

# Separate the independent variables (X) and the dependent variable (y)
y = X['Rating']
X = X.drop('Rating', axis=1)

# Add a constant column to the independent variables matrix
X = sm.add_constant(X)

# Perform multiple regression
regression = sm.OLS(y, X)
results_rating = regression.fit()

# Print the regression summary
print(results_rating.summary())
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                 Rating   R-squared:                       0.001
Model:                            OLS   Adj. R-squared:                 -0.006
Method:                 Least Squares   F-statistic:                   0.08496
Date:                Tue, 11 Jul 2023   Prob (F-statistic):              0.998
Time:                        19:42:16   Log-Likelihood:                -1959.7
No. Observations:                1000   AIC:                             3933.
Df Residuals:                     993   BIC:                             3968.
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
========================================================================================
                           coef    std err          t      P>|t|      [0.025      0.975]
----------------------------------------------------------------------------------------
const                 8.165e+12    9.3e+12      0.878      0.380   -1.01e+13    2.64e+13
Gender_Female         4.061e+12   4.48e+12      0.907      0.364   -4.72e+12    1.28e+13
Gender_Male           4.061e+12   4.48e+12      0.907      0.364   -4.72e+12    1.28e+13
Branch_A              1.592e+13   2.17e+13      0.735      0.462   -2.66e+13    5.84e+13
Branch_B              1.603e+12   5.91e+12      0.271      0.786   -9.99e+12    1.32e+13
Branch_C              1.339e+12   5.08e+12      0.264      0.792   -8.62e+12    1.13e+13
City_Mandalay         1.137e+12      5e+12      0.228      0.820   -8.67e+12    1.09e+13
City_Naypyitaw        1.401e+12   5.83e+12      0.240      0.810      -1e+13    1.28e+13
City_Yangon          -1.318e+13   1.17e+13     -1.129      0.259   -3.61e+13    9.73e+12
Customer_type_Member -9.543e+12   5.73e+12     -1.666      0.096   -2.08e+13     1.7e+12
Customer_type_Normal -9.543e+12   5.73e+12     -1.666      0.096   -2.08e+13     1.7e+12
Payment_Cash         -5.424e+12   3.19e+12     -1.703      0.089   -1.17e+13    8.27e+11
Payment_Credit card  -5.424e+12   3.19e+12     -1.703      0.089   -1.17e+13    8.27e+11
Payment_Ewallet      -5.424e+12   3.19e+12     -1.703      0.089   -1.17e+13    8.27e+11
==============================================================================
Omnibus:                      467.250   Durbin-Watson:                   1.985
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               53.881
Skew:                           0.013   Prob(JB):                     1.99e-12
Kurtosis:                       1.863   Cond. No.                     9.28e+18
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 3.48e-35. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
# Coefficient Plot
plt.figure(figsize=(20, 2))
coef_values = results_rating.params.drop('const')
coef_values.plot(kind='bar')
plt.title('Rating Regression Coefficient Plot')
plt.xlabel('Independent Variables')
plt.ylabel('Coefficient')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

-------

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv('/Users/isabellawhite/Desktop/Cornell/Machine Learning/Data_supermarket_sales.csv')
# Create subplots for each category
fig, axes = plt.subplots(3, 1, figsize=(10, 20))

# Calculate the total count for each category
total_counts = df['Gender'].count()
total_counts_by_city = df['City'].count()
total_counts_by_customer_type = df['Customer_type'].count()

# Plot the counts as a percentage of the total count for each category
categories = ['Gender', 'City', 'Customer_type']
for i, category in enumerate(categories):
    ax = axes[i]
    counts = df[category].value_counts()
    percent_counts = (counts / total_counts) * 100
    ax.bar(percent_counts.index, percent_counts.values)
    ax.set_title(category)
    ax.set_xlabel('Categories')
    ax.set_ylabel('Percentage')
    
    # Add data labels to the bars
    for j, value in enumerate(percent_counts.values):
        ax.text(j, value + 1, f"{value:.1f}%", ha='center', va='bottom')

    # Rotate x-axis labels if needed
    if len(counts) > 6:
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')

# Adjust the layout and spacing
plt.tight_layout()

# Display the plots
plt.show()

------------

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv('/Users/isabellawhite/Desktop/Cornell/Machine Learning/Data_supermarket_sales.csv')
import folium

# Group the data by City and calculate the sum of Total
total_by_city = df.groupby('City')['Total'].sum().reset_index()

# Create a folium map centered on Myanmar
myanmar_map = folium.Map(location=[21.9162, 95.9560], zoom_start=6)

# Create a choropleth map layer based on the Total by City
folium.Choropleth(
    geo_data='state_region.geojson',
    data=total_by_city,
    columns=['City', 'Total'],
    key_on='feature.properties.ST',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Total'
).add_to(myanmar_map)

# Display the map
myanmar_map
Make this Notebook Trusted to load map: File -> Trust Notebook
 
---------------

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv('/Users/isabellawhite/Desktop/Cornell/Machine Learning/Data_supermarket_sales.csv')
# Convert 'Date' column to datetime type
df['Date'] = pd.to_datetime(df['Date'])

# Set 'Date' column as the DataFrame index
df.set_index('Date', inplace=True)

# Define the resampling period
resample_period = 'D'

# Resample the data based on the defined period and calculate the sum of 'Total'
resampled_totals = df['Total'].resample(resample_period).sum()

# Determine the title based on the resampling period
if resample_period == 'D':
    title = 'Sum of Daily Sales over Time'
elif resample_period == 'W':
    title = 'Sum of Weekly Sales over Time'
else:
    title = f'Sum of Sales ({resample_period}) over Time'

# Plot the sum of Total each period over time
plt.figure(figsize=(10, 6))
plt.plot(resampled_totals.index, resampled_totals)
plt.title(title)
plt.xlabel('Date')
plt.ylabel('Total')
plt.xticks(rotation=45)
plt.grid(True)

# Display the plot
plt.show()

# Convert the "Date" column to pandas datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Create a new DataFrame with the sum of "Total" per day by Branch
total_per_day_by_branch = df.groupby(['Date', 'Branch'])['Total'].sum().reset_index()

# Print the new DataFrame
print(total_per_day_by_branch)
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.statespace.sarimax import SARIMAX

# Convert the "Date" column to pandas datetime format
total_per_day_by_branch['Date'] = pd.to_datetime(total_per_day_by_branch['Date'])

# Set the "Date" column as the index
total_per_day_by_branch.set_index('Date', inplace=True)

# Aggregate duplicate values by taking the mean
total_per_day_by_branch = total_per_day_by_branch.groupby(level=0).mean()

# Set the frequency of the time series to daily
total_per_day_by_branch = total_per_day_by_branch.asfreq('D')

# Perform seasonal decomposition
decomposition = seasonal_decompose(total_per_day_by_branch['Total'], model='additive')

# Get the trend, seasonal, and residual components
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

# Create a SARIMA model and fit it to the data
model = SARIMAX(total_per_day_by_branch['Total'], order=(1, 0, 0), seasonal_order=(1, 1, 1, 7))
model_fit = model.fit()

# Forecast the next 30 days
forecast = model_fit.get_forecast(steps=30)

# Get the forecasted values
forecasted_total = forecast.predicted_mean

# Create a date range for the forecast period
start_date = total_per_day_by_branch.index[-1] + pd.DateOffset(days=1)
end_date = start_date + pd.DateOffset(days=29)
forecast_dates = pd.date_range(start_date, end_date)

# Create a DataFrame to store the forecasted values
forecast_df = pd.DataFrame({
    'Date': forecast_dates,
    'Forecasted Total': forecasted_total
})

# Print the forecasted values
print(forecast_df)
RUNNING THE L-BFGS-B CODE

           * * *

Machine precision = 2.220D-16
 N =            4     M =           10

At X0         0 variables are exactly at the bounds

At iterate    0    f=  7.30510D+00    |proj g|=  3.55558D-02

At iterate    5    f=  7.28338D+00    |proj g|=  1.16646D-02

At iterate   10    f=  7.28304D+00    |proj g|=  2.89824D-03

At iterate   15    f=  7.22892D+00    |proj g|=  1.15863D-01

At iterate   20    f=  7.21497D+00    |proj g|=  7.77891D-05

           * * *

Tit   = total number of iterations
Tnf   = total number of function evaluations
Tnint = total number of segments explored during Cauchy searches
Skip  = number of BFGS updates skipped
Nact  = number of active bounds at final generalized Cauchy point
Projg = norm of the final projected gradient
F     = final function value

           * * *

   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F
    4     21     26      1     0     0   1.921D-06   7.215D+00
  F =   7.2149715934283556     

CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            
                 Date  Forecasted Total
2019-03-31 2019-03-31       1204.871454
2019-04-01 2019-04-01       1014.300422
2019-04-02 2019-04-02       1255.196735
2019-04-03 2019-04-03       1161.640301
2019-04-04 2019-04-04       1114.683213
2019-04-05 2019-04-05       1210.771969
2019-04-06 2019-04-06       1418.145852
2019-04-07 2019-04-07       1210.597054
2019-04-08 2019-04-08       1029.829380
2019-04-09 2019-04-09       1291.803787
2019-04-10 2019-04-10       1173.457196
2019-04-11 2019-04-11       1137.311246
2019-04-12 2019-04-12       1202.835910
2019-04-13 2019-04-13       1413.423484
2019-04-14 2019-04-14       1210.945754
2019-04-15 2019-04-15       1030.775122
2019-04-16 2019-04-16       1294.033224
2019-04-17 2019-04-17       1174.176867
2019-04-18 2019-04-18       1138.689335
2019-04-19 2019-04-19       1202.352589
2019-04-20 2019-04-20       1413.135883
2019-04-21 2019-04-21       1210.966990
2019-04-22 2019-04-22       1030.832720
2019-04-23 2019-04-23       1294.169001
2019-04-24 2019-04-24       1174.220696
2019-04-25 2019-04-25       1138.773263
2019-04-26 2019-04-26       1202.323154
2019-04-27 2019-04-27       1413.118368
2019-04-28 2019-04-28       1210.968284
2019-04-29 2019-04-29       1030.836227
 This problem is unconstrained.

-----------
